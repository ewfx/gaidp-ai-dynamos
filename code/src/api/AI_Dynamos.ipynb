{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ngrok\n",
        "!pip install flask-cors\n",
        "!pip install pyngrok\n",
        "!pip install pdfplumber\n",
        "!pip install ftfy\n",
        "!pip install scikit-learn"
      ],
      "metadata": {
        "id": "eJf6n25vkrj9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6525e73-fdb8-4180-b5ab-08a0264aee2a",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ngrok in /usr/local/lib/python3.11/dist-packages (1.4.0)\n",
            "Requirement already satisfied: flask-cors in /usr/local/lib/python3.11/dist-packages (5.0.1)\n",
            "Requirement already satisfied: flask>=0.9 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.11/dist-packages (from flask-cors) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (6.3.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy) (0.2.13)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "q0ofbj6JEV4-",
        "outputId": "93a21d94-06e6-4a1f-d565-fb753c999ff6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt_tab is already up-to-date!\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import pdfplumber\n",
        "import ftfy\n",
        "from transformers import pipeline\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import nltk\n",
        "import unicodedata\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Download NLTK sentence tokenizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Initialize the model\n",
        "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
        "\n",
        "def summarize_with_embeddings(text):\n",
        "    \"\"\"Generate embeddings for each sentence and return the most representative ones.\"\"\"\n",
        "    # Tokenize sentences properly\n",
        "    sentences = sent_tokenize(text)\n",
        "\n",
        "    # Generate embeddings for each sentence\n",
        "    sentence_embeddings = model.encode(sentences)\n",
        "\n",
        "    # Compute the overall document embedding as the mean of all sentence embeddings\n",
        "    doc_embedding = np.mean(sentence_embeddings, axis=0).reshape(1, -1)\n",
        "\n",
        "    # Compute cosine similarity between each sentence embedding and the document embedding\n",
        "    similarities = cosine_similarity(sentence_embeddings, doc_embedding).flatten()\n",
        "\n",
        "    # Select the top 2 most representative sentences\n",
        "    top_indices = similarities.argsort()[-2:][::-1]\n",
        "    summary = ' '.join([sentences[i] for i in top_indices])\n",
        "\n",
        "    return summary\n",
        "\n",
        "def extract_tech_name(Field_Name):\n",
        "    # Original string\n",
        "    text = Field_Name\n",
        "    extracted_text = \"\"\n",
        "    # Regular expression to match text inside parentheses\n",
        "    matches = re.findall(r'\\((.*?)\\)', Field_Name, re.DOTALL)\n",
        "\n",
        "    # Check if a match is found and print the result\n",
        "    if matches:\n",
        "        extracted_text = matches[-1].replace(\"\\n\", \"\")\n",
        "        #print(extracted_text)\n",
        "    else:\n",
        "        print(\"No match found\")\n",
        "\n",
        "    cleaned_text = extracted_text.replace(\" \", \"\")\n",
        "    return cleaned_text\n",
        "\n",
        "def clean_newline(text_field):\n",
        "    # Original string\n",
        "    text = text_field\n",
        "\n",
        "    cleaned_text = text.replace(\"\\n\", \" \")\n",
        "    return cleaned_text\n",
        "\n",
        "def validate_psrlei(psr, obligor, lei):\n",
        "    \"\"\"\n",
        "    Validate Primary Source of Repayment LEI (PSRLEI) based on FRY14Q rules using LLM.\n",
        "    :param psr: Primary Source of Repayment (Field 50)\n",
        "    :param obligor: Obligor (Field 2 or 4)\n",
        "    :param lei: LEI provided (Field 112)\n",
        "    :return: Validation result (Valid or Error Message)\n",
        "    \"\"\"\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt-3.5-turbo\")\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Given the following inputs:\n",
        "    - Primary Source of Repayment (PSR): {psr}\n",
        "    - Obligor: {obligor}\n",
        "    - LEI: {lei}\n",
        "\n",
        "    Apply the following validation rules:\n",
        "    1. If PSR = Obligor, LEI must be blank.\n",
        "    2. If PSR ≠ Obligor, LEI must be a valid 20-character alphanumeric code or 'NA'.\n",
        "    3. If LEI is invalid, return an appropriate error message.\n",
        "\n",
        "    Return 'Valid' if all rules are met, otherwise return the specific error message.\n",
        "    \"\"\"\n",
        "\n",
        "    result = generator(prompt, max_length=100, do_sample=False)[0]['generated_text']\n",
        "    return result\n",
        "\n",
        "def extract_tables_from_pdf(pdf_path, schedule=\"H.1 - Corporate Loan Data Schedule\", start_text=\"H.1 - Corporate Loan Data Schedule\", end_text=\"H.2 – Commercial Real Estate Schedule\"):\n",
        "\n",
        "    \"\"\"Extracts tables only from the specified schedule (H.1) between start and end markers.\"\"\"\n",
        "    table_data = []\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        extract = False\n",
        "        for page in pdf.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                if start_text in text:\n",
        "                    extract = True\n",
        "                if extract:\n",
        "                    tables = page.extract_table()\n",
        "                    if tables:\n",
        "                        table_data.extend(tables)  # Collect all table rows\n",
        "                if end_text in text:\n",
        "                    extract = False\n",
        "                    break\n",
        "    return table_data\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Cleans extracted text by removing backslashes, forward slashes, and extra spaces.\"\"\"\n",
        "    if text:\n",
        "        return re.sub(r\"[\\\\/]+\", \"\", text).strip()\n",
        "    return \"\"\n",
        "\n",
        "def process_table_data(table_data):\n",
        "    \"\"\"Processes extracted table data into structured fields with complete descriptions.\"\"\"\n",
        "    # print(f\"Number of rows in the table data: {len(table_data)}\")\n",
        "    fields = {}\n",
        "    i=0\n",
        "    for row in table_data:\n",
        "        i=i+1\n",
        "        if len(row) <= 5:  # Ensuring the row has enough columns\n",
        "            field_no, field_name, technical_name, description, constraints = map(clean_text, row[:5])\n",
        "\n",
        "            if field_name and technical_name:  # Ensure valid field entries\n",
        "\n",
        "                if field_name in fields:  # If field already exists, append to description (handles multi-line cases)\n",
        "                    fields[field_name][\"description\"] += \" \" + description\n",
        "                else:\n",
        "                    fields[field_name] = {\n",
        "                        \"field_no\": field_no,\n",
        "                        \"field_name\": clean_newline(field_name),\n",
        "                        \"technical_name\": technical_name,\n",
        "                        \"description\": clean_newline(unicodedata.normalize(\"NFKC\", re.sub(r'\\s+', ' ', bytes(ftfy.fix_text(description), \"utf-8\").decode(\"utf-8\", \"ignore\")).strip())),\n",
        "                        \"constraints\": clean_newline(unicodedata.normalize(\"NFKC\", re.sub(r'\\s+', ' ', bytes(ftfy.fix_text(constraints), \"utf-8\").decode(\"utf-8\", \"ignore\")).strip())),\n",
        "                         \"mandatory\": True,  # Assuming all fields are mandatory\n",
        "                    }\n",
        "    return fields\n",
        "\n",
        "def generate_rule_dictionary(fields):\n",
        "    \"\"\"Generates validation rules dynamically based on extracted fields.\"\"\"\n",
        "    rule_dict = {}\n",
        "    # print(f\"Length of fields: {len(fields)}\")\n",
        "    for field, properties in fields.items():\n",
        "        # print(\"Field No, FieldName\")\n",
        "        # print(properties[\"field_no\"],properties[\"field_name\"])\n",
        "\n",
        "        rule_dict[field] = {\n",
        "            \"field_no\": properties[\"field_no\"],\n",
        "            \"field_name\": properties[\"field_name\"],\n",
        "            \"technical_name\": extract_tech_name(properties[\"field_name\"]),\n",
        "            \"description\": properties[\"description\"],\n",
        "            \"allowable_values\": properties[\"constraints\"],\n",
        "            \"validation_rules\": summarize_with_embeddings(properties[\"description\"])+\"\\n\"+properties[\"constraints\"]\n",
        "\n",
        "        }\n",
        "    return rule_dict"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0gB51AMQeTTs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load JSON file with rules\n",
        "def load_rules(file_path):\n",
        "    with open(file_path, \"r\") as file:\n",
        "        return json.load(file)\n",
        "\n",
        "rules_data = load_rules(\"FRY14QH1_rules.txt\")\n",
        "\n",
        "# Extract validation rules into a DataFrame\n",
        "rules = []\n",
        "for field, details in rules_data[\"Corporate Loan Data Fields\"].items():\n",
        "    rules.append({\n",
        "        \"Field\": field,\n",
        "        \"Condition\": details[\"validation\"],\n",
        "        \"Valid\": 1,\n",
        "        \"Valid_Example\": details[\"valid_example\"],\n",
        "        \"Invalid_Example\": details[\"invalid_example\"]\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rules)\n",
        "\n",
        "def extract_features(text, valid_example, invalid_example):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "    return {\n",
        "        \"num_tokens\": len(tokens),\n",
        "        \"num_nouns\": len([word for word, pos in pos_tags if pos.startswith(\"NN\")]),\n",
        "        \"num_verbs\": len([word for word, pos in pos_tags if pos.startswith(\"VB\")]),\n",
        "        \"valid_example_length\": len(valid_example) if isinstance(valid_example, str) else 0,\n",
        "        \"invalid_example_count\": len(invalid_example) if isinstance(invalid_example, list) else 0\n",
        "    }\n",
        "\n",
        "features_df = pd.DataFrame(df.apply(lambda row: extract_features(str(row['Condition']), row['Valid_Example'], row['Invalid_Example']), axis=1).tolist())\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_df, df['Valid'], test_size=0.2, random_state=42)\n",
        "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Function to validate individual field values\n",
        "def validate_input(field_name, value, internal_id=None):\n",
        "    errors = []\n",
        "    field_info = rules_data[\"Corporate Loan Data Fields\"].get(field_name, {})\n",
        "\n",
        "    if not field_info:\n",
        "        return \"Error: No validation rules found.\"\n",
        "\n",
        "    validation_rule = field_info.get(\"validation\", \"\")\n",
        "    valid_example = field_info.get(\"valid_example\", \"No example available.\")\n",
        "    invalid_example = field_info.get(\"invalid_example\", [\"No invalid examples.\"])\n",
        "\n",
        "    if not value or pd.isna(value):\n",
        "        errors.append(f\"'{field_name}' cannot be empty.\")\n",
        "\n",
        "    value = str(value).strip()\n",
        "\n",
        "    if \"must be a whole number\" in validation_rule.lower() and not re.fullmatch(r\"^-?\\d+$\", value):\n",
        "        errors.append(f\"'{value}' must be a valid whole number.\")\n",
        "    if \"YYYY-MM-DD\" in validation_rule and not re.fullmatch(r\"^\\d{4}-\\d{2}-\\d{2}$\", value):\n",
        "        errors.append(f\"'{value}' must be in YYYY-MM-DD format.\")\n",
        "    if \"must be uppercase\" in validation_rule.lower() and not value.isupper():\n",
        "        errors.append(f\"'{value}' must be uppercase.\")\n",
        "    if \"max 20 characters\" in validation_rule.lower() and len(value) > 20:\n",
        "        errors.append(f\"'{value}' exceeds the max length of 20 characters.\")\n",
        "    if \"must be uppercase\" in validation_rule.lower() and not value.isupper():\n",
        "        errors.append(f\"'{value}' must be uppercase.\")\n",
        "\n",
        "    if \"must be exactly 2 characters\" in validation_rule.lower() and not re.fullmatch(r\"^[A-Z]{2}$\", value):\n",
        "        errors.append(f\"'{value}' must be exactly 2 characters (ISO 3166-1 alpha-2 country code).\")\n",
        "\n",
        "    if \"YYYY-MM-DD\" in validation_rule and not re.fullmatch(r\"^\\d{4}-\\d{2}-\\d{2}$\", value):\n",
        "        errors.append(f\"'{value}' must be in YYYY-MM-DD format.\")\n",
        "\n",
        "    if \"must be a future date\" in validation_rule and value <= \"2024-03-26\":\n",
        "        errors.append(f\"'{value}' must be a future date.\")\n",
        "\n",
        "    if \"must be a past or present date\" in validation_rule and value > \"2024-03-26\":\n",
        "        errors.append(f\"'{value}' must be a past or present date.\")\n",
        "\n",
        "    if \"decimal format\" in validation_rule.lower() and not re.fullmatch(r\"^\\d+(\\.\\d+)?$\", value):\n",
        "        errors.append(f\"'{value}' must be a valid decimal number.\")\n",
        "\n",
        "    if \"percentage format\" in validation_rule.lower() and not re.fullmatch(r\"^\\d{1,3}(\\.\\d{1,2})?$\", value):\n",
        "        errors.append(f\"'{value}' must be a valid percentage (0-100, max 2 decimals).\")\n",
        "\n",
        "    if field_name == \"Country\" and not re.fullmatch(r\"^[A-Z]{2}$\", value):\n",
        "        errors.append(f\"'{value}' must be a valid 2-letter country code (ISO 3166-1). Example: 'US'.\")\n",
        "\n",
        "    if \"must be alphanumeric\" in validation_rule.lower() and not re.fullmatch(r\"^[a-zA-Z0-9]+$\", value):\n",
        "        errors.append(f\"'{value}' must be alphanumeric, with no special characters.\")\n",
        "\n",
        "    if \"max 20 characters\" in validation_rule.lower() and len(value) > 20:\n",
        "        errors.append(f\"'{value}' exceeds the maximum length of 20 characters.\")\n",
        "\n",
        "    if \"must match format of Internal_ID\" in validation_rule.lower() and internal_id is not None:\n",
        "        if not re.fullmatch(r\"^[a-zA-Z0-9]+$\", value) or len(value) != len(internal_id):\n",
        "            errors.append(f\"'{value}' must match the format and length of Internal_ID '{internal_id}'.\")\n",
        "    if \"cannot contain spaces or special characters\" in validation_rule.lower() and not re.fullmatch(r\"^[a-zA-Z0-9]+$\", value):\n",
        "        errors.append(f\"'{value}' cannot contain spaces or special characters.\")\n",
        "    if \"max 50 characters\" in validation_rule.lower() and len(value) > 50:\n",
        "        errors.append(f\"'{value}' exceeds the maximum length of 50 characters.\")\n",
        "    if \"max 10 characters\" in validation_rule.lower() and len(value) > 10:\n",
        "        errors.append(f\"'{value}' exceeds the maximum length of 10 characters.\")\n",
        "    if \"max 100 characters\" in validation_rule.lower() and len(value) > 100:\n",
        "        errors.append(f\"'{value}' exceeds the maximum length of 100 characters.\")\n",
        "    if \"must be a whole number\" in validation_rule.lower():\n",
        "        if not re.fullmatch(r\"^\\d+$\", value):\n",
        "            errors.append(f\"'{value}' must be a whole number with no decimals, commas, or symbols.\")\n",
        "        elif int(value) < 0:\n",
        "            errors.append(f\"'{value}' cannot be negative.\")\n",
        "    if \"decimal format\" in validation_rule.lower():\n",
        "        if not re.fullmatch(r\"^0(\\.\\d{1,7})?$|^1(\\.0{1,7})?$\", value):\n",
        "            errors.append(f\"'{value}' must be a decimal number between 0 and 1 with up to 7 decimal places.\")\n",
        "        elif not (0 <= float(value) <= 1):\n",
        "            errors.append(f\"'{value}' must be between 0 and 1.\")\n",
        "    if \"decimal format\" in validation_rule.lower():\n",
        "        if not re.fullmatch(r\"^\\d{1,3}(\\.\\d{1,4})?$\", value):\n",
        "            errors.append(f\"'{value}' must be a decimal number with up to 4 decimal places, between 0 and 100.\")\n",
        "        elif not (0 <= float(value) <= 100):\n",
        "            errors.append(f\"'{value}' must be between 0 and 100.\")\n",
        "    if field_name == \"Currency\" and not re.fullmatch(r\"^[A-Z]{3}$\", value):\n",
        "        errors.append(f\"'{value}' must be a valid ISO 4217 three-letter currency code in uppercase.\")\n",
        "    if field_name == \"Currency\" and not re.fullmatch(r\"^[A-Z]{3}$\", value):\n",
        "        errors.append(f\"'{value}' must be a valid ISO 4217 three-letter currency code in uppercase.\")\n",
        "    if \"enumerated values\" in validation_rule.lower():\n",
        "            valid_options = re.findall(r\"\\((.*?)\\)\", validation_rule)\n",
        "            if valid_options:\n",
        "                valid_options = [opt.strip() for opt in valid_options[0].split(\",\")]\n",
        "                if value not in valid_options:\n",
        "                    errors.append(f\"'{value}' is not a valid option. Allowed values: {', '.join(valid_options)}.\")\n",
        "    if errors:\n",
        "        return \"Non-Compliant\", errors\n",
        "\n",
        "    # Machine Learning validation\n",
        "    input_features = extract_features(validation_rule, valid_example, invalid_example)\n",
        "    input_df = pd.DataFrame([input_features])\n",
        "    prediction = clf.predict(input_df)[0]\n",
        "\n",
        "    return (\"Compliant\" if prediction == 1 else \"Non-Compliant (ML Prediction)\", [])\n",
        "\n",
        "# Function to validate an entire XLS file\n",
        "def validate_xls(file_path):\n",
        "    df = pd.read_excel(file_path)\n",
        "    anomalies = {col: [] for col in df.columns}\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        for field_name, value in row.items():\n",
        "            compliance_status, issues = validate_input(field_name, value)\n",
        "            anomalies[field_name].append((value, compliance_status, \"; \".join(issues)))\n",
        "\n",
        "    anomalies_df = pd.DataFrame({col: [list(x) for x in zip(*values)] for col, values in anomalies.items()},\n",
        "                                index=[\"Value\", \"Compliance_Status\", \"Issues\"]).T\n",
        "\n",
        "    # anomalies_df.to_excel(\"anomalies_report.xlsx\", index=True)\n",
        "    # anomalies_df.to_json(\"anomalies_report.json\", orient=\"records\", indent=4)\n",
        "    json_data = anomalies_df.reset_index().to_json(orient=\"records\", indent=4)\n",
        "    return json_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XIRr8eMuCyy7",
        "outputId": "367572ee-afcd-4ce6-a3a7-f7fe96f02ae9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "    {\n",
            "        \"index\":\"Customer_ID\",\n",
            "        \"Value\":[\n",
            "            \"123456789012345ABCD\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Internal_ID\",\n",
            "        \"Value\":[\n",
            "            \"4f762d87-5c8a-430a-a18a-e9b9c1b7073f\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'4f762d87-5c8a-430a-a18a-e9b9c1b7073f' exceeds the max length of 20 characters.; '4f762d87-5c8a-430a-a18a-e9b9c1b7073f' must be alphanumeric, with no special characters.; '4f762d87-5c8a-430a-a18a-e9b9c1b7073f' exceeds the maximum length of 20 characters.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Original_Internal_ID\",\n",
            "        \"Value\":[\n",
            "            \"62956d9f-0445-4566-86ac-960cdc8db76b\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'62956d9f-0445-4566-86ac-960cdc8db76b' cannot contain spaces or special characters.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Obligor_Name\",\n",
            "        \"Value\":[\n",
            "            \"ABC\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"City\",\n",
            "        \"Value\":[\n",
            "            \"City_29\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Country\",\n",
            "        \"Value\":[\n",
            "            \"CA\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Zip_Code\",\n",
            "        \"Value\":[\n",
            "            63725\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Facility_ID\",\n",
            "        \"Value\":[\n",
            "            \"b18822fd-96d9-4179-b1d2-185740ebdf8c\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'b18822fd-96d9-4179-b1d2-185740ebdf8c' exceeds the max length of 20 characters.; 'b18822fd-96d9-4179-b1d2-185740ebdf8c' exceeds the maximum length of 20 characters.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Loan_Type\",\n",
            "        \"Value\":[\n",
            "            \"Bridge Loan\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'Bridge Loan' is not a valid option. Allowed values: Term Loan, Revolving, etc..\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Currency\",\n",
            "        \"Value\":[\n",
            "            \"CNY\"\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Commitment_Amount\",\n",
            "        \"Value\":[\n",
            "            408657315.1000000238\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'408657315.1' must be a valid whole number.; '408657315.1' must be a whole number with no decimals, commas, or symbols.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Outstanding_Balance\",\n",
            "        \"Value\":[\n",
            "            575918448.2000000477\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'575918448.2' must be a valid whole number.; '575918448.2' must be a whole number with no decimals, commas, or symbols.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Interest_Rate\",\n",
            "        \"Value\":[\n",
            "            1.34\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'1.34' must be a decimal number between 0 and 1 with up to 7 decimal places.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"Maturity_Date\",\n",
            "        \"Value\":[\n",
            "            1692835200000\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'2023-08-24 00:00:00' must be in YYYY-MM-DD format.; '2023-08-24 00:00:00' must be in YYYY-MM-DD format.; '2023-08-24 00:00:00' must be a future date.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"PD\",\n",
            "        \"Value\":[\n",
            "            894254\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'894254' must be a decimal number between 0 and 1 with up to 7 decimal places.; '894254' must be a decimal number with up to 4 decimal places, between 0 and 100.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"LGD\",\n",
            "        \"Value\":[\n",
            "            518639.4\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'518639.4' must be a valid percentage (0-100, max 2 decimals).\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"EAD\",\n",
            "        \"Value\":[\n",
            "            731181.39\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'731181.39' must be a valid whole number.; '731181.39' must be a whole number with no decimals, commas, or symbols.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"RWA\",\n",
            "        \"Value\":[\n",
            "            983217.3199999999\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'983217.32' must be a valid whole number.; '983217.32' must be a whole number with no decimals, commas, or symbols.\"\n",
            "        ]\n",
            "    },\n",
            "    {\n",
            "        \"index\":\"LTV\",\n",
            "        \"Value\":[\n",
            "            905994.2\n",
            "        ],\n",
            "        \"Compliance_Status\":[\n",
            "            \"Non-Compliant\"\n",
            "        ],\n",
            "        \"Issues\":[\n",
            "            \"'905994.2' must be a decimal number between 0 and 1 with up to 7 decimal places.; '905994.2' must be a decimal number with up to 4 decimal places, between 0 and 100.\"\n",
            "        ]\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import requests\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "import base64\n",
        "from flask_cors import CORS\n",
        "import json\n",
        "import os\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Load rules.json and send it as a response\n",
        "@app.route('/get_current_rules', methods=['GET'])\n",
        "def get_current_rules():\n",
        "    try:\n",
        "        with open(\"FRY14QH1_rules.json\", \"r\", encoding=\"utf-8\") as file:\n",
        "            rules_data = json.load(file)\n",
        "        return jsonify(rules_data), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# API to handle PDF file upload\n",
        "@app.route('/generate_new_rules', methods=['POST'])\n",
        "def upload_pdf():\n",
        "    try:\n",
        "        UPLOAD_FOLDER = \"pdfUploads\"\n",
        "        os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "        file = request.files['file']\n",
        "        # Save PDF file\n",
        "        pdf_path = os.path.join(UPLOAD_FOLDER, \"generate_rules.pdf\")\n",
        "        file.save(pdf_path)\n",
        "        print(f\"Received PDF: {file.filename}\")\n",
        "\n",
        "        pdf_table_data = extract_tables_from_pdf(pdf_path, schedule=\"H.1 - Corporate Loan Data Schedule\", start_text=\"H.1 - Corporate Loan Data Schedule\", end_text=\"H.2 – Commercial Real Estate Schedule\")\n",
        "        extracted_fields = process_table_data(pdf_table_data)\n",
        "        rule_dictionary = generate_rule_dictionary(extracted_fields)\n",
        "        json_text = json.dumps(rule_dictionary, indent=4)\n",
        "\n",
        "        file_path = \"FRY14QH1_rules.json\"\n",
        "        with open(file_path, \"w\") as json_file:\n",
        "            json.dump(json_text, json_file, indent=4)\n",
        "\n",
        "        print(f\"JSON data has been written to {file_path}\")\n",
        "        return jsonify({'status': 'success', 'message': 'Rules are successfully generated.'}), 200\n",
        "\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "# API to generate anomalies report\n",
        "@app.route('/generate_anomalies_report', methods=['POST'])\n",
        "def upload_excel():\n",
        "    try:\n",
        "        UPLOAD_FOLDER = \"sampleDataUploads\"\n",
        "        os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
        "        file = request.files[\"file\"]\n",
        "        file_path = os.path.join(UPLOAD_FOLDER, file.filename)\n",
        "        file.save(file_path)\n",
        "        df = pd.read_excel(file_path)\n",
        "        json_data = validate_xls(file_path)\n",
        "\n",
        "        return jsonify({\"status\": \"success\", \"data\": json_data}), 200\n",
        "    except Exception as e:\n",
        "        return jsonify({'error': str(e)}), 500\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Set ngrok auth token\n",
        "    ngrok.set_auth_token(\"2UCBnO3oWGPJqVUOcUQ1yDWtUCY_4eizetwxTW6ZoesLiA2wG\")\n",
        "    # Start ngrok tunnel\n",
        "    public_url = ngrok.connect(5000)\n",
        "    print('Public URL:', public_url)\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "f5Vw7HiakQ3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcdeea58-d451-4f0b-86b7-59d067231f6a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://2489-34-57-49-145.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on all addresses (0.0.0.0)\n",
            " * Running on http://127.0.0.1:5000\n",
            " * Running on http://172.28.0.12:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 17:00:16] \"GET /get_current_rules HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Received PDF: FR_Y-14Q20240331.pdf\n",
            "No match found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 17:04:18] \"POST /generate_new_rules HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JSON data has been written to FRY14QH1_rules.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 17:04:22] \"GET /get_current_rules HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [26/Mar/2025 17:05:10] \"POST /generate_anomalies_report HTTP/1.1\" 200 -\n"
          ]
        }
      ]
    }
  ]
}